name: Crawl Exam Questions

on:
  schedule:
    # 매일 자정(UTC)에 실행
    - cron: '0 0 * * *'
  workflow_dispatch:
    # 수동 실행 허용 (운영 데이터 강제 갱신용)

jobs:
  crawl:
    name: Crawl and Update DB
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip' # 패키지 캐싱으로 속도 향상

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r services/crawler/requirements.txt

      - name: Setup Chrome
        uses: browser-actions/setup-chrome@latest
        with:
          chrome-version: stable

      - name: Run Crawler
        id: run-crawler
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          IS_CI: 'true'
        run: |
          python services/crawler/main.py
          echo "### ✅ Crawling Completed" >> $GITHUB_STEP_SUMMARY
          echo "The questions have been successfully updated in Supabase." >> $GITHUB_STEP_SUMMARY
